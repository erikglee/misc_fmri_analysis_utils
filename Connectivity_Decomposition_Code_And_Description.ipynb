{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools for Decomposing Functional Connectivity Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains functions for decomposing functional connectivity maps into different components (either using PCA, Dictionary Learning, or ICA as implemented in scikit-learn. The only non-standard package required to run these scripts is nibabel, which can be installed without sudo by running \"pip install nibabel --user\" in the command line. This code has been developed/tested using Python3.6. \n",
    "\n",
    "See the text below the functions to get more information on usage/functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import decomposition\n",
    "from scipy.stats import zscore\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conns(path_or_list, parcel_ids_path=None):\n",
    "    \"\"\"\n",
    "    #Function to assist in loading connectivity values.\n",
    "    #This function supports a few different functionalities.\n",
    "    #The input \"path_or_list\" can either be a path or list to\n",
    "    #supported data types.\n",
    "    \n",
    "    #Supported data types include (1) pscalar files, (2) csv\n",
    "    #files, (3) numpy arrays. In the (1) pscalar usage, path_or_list\n",
    "    #can either be a path to a folder containing ONLY the pscalar files\n",
    "    #you want to load. In the (2) csv case, path_or_list should be a string\n",
    "    #that points to a csv file where the first column is named 'subject_id',\n",
    "    #and the subsequent columns are headings for the connectivity edges\n",
    "    #represented by subsequent rows. In the (3) case, similar to the (1)\n",
    "    #case, 'path_or_list' can either be a path to a folder containing\n",
    "    #ONLY *.npy files which are arrays to represent connectivity, AND optionally\n",
    "    #a SINGLE *.txt file with one entry per line that specifies names for\n",
    "    #different connectivity edges. The *.txt file is optional.\n",
    "    \n",
    "    #This function will then output a matrix shape <num subjects, num edges>,\n",
    "    #a list of parcel_labels (if found), and a template cifti file that can\n",
    "    #be used to help create visualizations (if pscalar.nii input option is\n",
    "    #used)\n",
    "    \"\"\"\n",
    "    \n",
    "    starting_dir = os.getcwd()\n",
    "    \n",
    "    conns_list = []\n",
    "    files_list = []\n",
    "    template_cifti_path = None\n",
    "    load_csv = False\n",
    "    \n",
    "    if type(path_or_list) == list:\n",
    "        \n",
    "        files_to_load = path_or_list\n",
    "        subjects = files_to_load\n",
    "        \n",
    "    if type(path_or_list) == str:\n",
    "        \n",
    "        if path_or_list[-3:] == 'csv':\n",
    "            \n",
    "            load_csv = True\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            os.chdir(path_or_list)\n",
    "            nifti_files = glob.glob('*.nii')\n",
    "            npy_files = glob.glob('*.npy')\n",
    "            \n",
    "            if len(nifti_files) > 1:\n",
    "                \n",
    "                files_to_load = nifti_files\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                files_to_load = npy_files\n",
    "                \n",
    "            subjects = []\n",
    "            for temp_file in files_to_load:\n",
    "                subjects.append(temp_file.split('/')[-1])\n",
    "            \n",
    "    \n",
    "    \n",
    "    if load_csv:\n",
    "        \n",
    "        df = pd.read_csv(path_or_list)\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        subjects = df.subject_id.values\n",
    "        parcel_labels = df.columns[1:].values\n",
    "        conn_inds = df.columns[1:]\n",
    "        subj_conn_vals = df[conn_inds].to_numpy()\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        conn_list = []\n",
    "        \n",
    "        if files_to_load[0][-3:] == 'nii':\n",
    "            \n",
    "            #Iterate through all cifti files\n",
    "            for i, temp_file_path in enumerate(files_to_load):\n",
    "                \n",
    "                temp_cifti = nib.load(temp_file_path)\n",
    "                \n",
    "                #Grab the parcel labels from cifti header\n",
    "                if i == 0:\n",
    "                    \n",
    "                    parcel_labels = temp_cifti.header.get_axis(1).name\n",
    "                    template_cifti_path = os.path.join(path_or_list, temp_file_path)\n",
    "                  \n",
    "                conn_list.append(temp_cifti.get_fdata())\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            for temp_file in files_to_load:\n",
    "                \n",
    "                temp_out = np.load()\n",
    "                conn_list.append(temp_out.copy())\n",
    "                            \n",
    "            if type(parcel_ids_path) == type(None):\n",
    "                \n",
    "                os.chdir(path_or_list)\n",
    "                parcel_ids_path = glob.glob('*txt')[0]\n",
    "            \n",
    "            with open(parcel_ids_path, 'r') as parcel_file:\n",
    "                \n",
    "                parcel_labels = parcel_file.read().split('\\n')\n",
    "                \n",
    "        #Convert list to numpy matrix   \n",
    "        subj_conn_vals = np.vstack(conn_list)\n",
    "                \n",
    "                \n",
    "                \n",
    "    return subj_conn_vals, parcel_labels, subjects, template_cifti_path\n",
    "\n",
    "\n",
    "def make_pscalar_cifti(data_array, template_cifti_path, output_cifti_path, dimension_names=None):\n",
    "    \"\"\"\n",
    "    #Function that creates a cifti pscalar.nii file for\n",
    "    #visualizing overlays. data_array should  have a shape\n",
    "    #<n_dimensions, n_regions> where n_dimensions is at least\n",
    "    #1. template_cifti_path is the path to a cifti file (either\n",
    "    #pscalar or ptseries works) that has the same parcel definitions\n",
    "    #as desired for visualizing the data_array. Output_cifti_path\n",
    "    #specifies the name of the file to be saved (should end in\n",
    "    #*.pscalar.nii). dimension_names is optional list that will serve\n",
    "    #as the name of different dimensions (i.e. PC1, PC2, etc.). If\n",
    "    #specified, dimension_names should have one element for each dimension\n",
    "    #(not region) in data_array. Function saves new file to output path,\n",
    "    #doesn't return anything\n",
    "    \"\"\"\n",
    "    \n",
    "    #Set dimension names if not specified\n",
    "    if type(dimension_names) == type(None):\n",
    "        dimension_names = ['NONAME']*data_array.shape[0]\n",
    "    \n",
    "    #Load template cifti file\n",
    "    template_cifti = nib.load(template_cifti_path)\n",
    "    \n",
    "    #Copy/generate axis for new cifti file\n",
    "    pscalar_axis_1 = template_cifti.header.get_axis(1)\n",
    "    pscalar_axis_0 = nib.cifti2.cifti2_axes.ScalarAxis(dimension_names)\n",
    "    \n",
    "    #Put the axes into a header\n",
    "    cifti_hdr = nib.cifti2.cifti2_axes.to_header([pscalar_axis_0, pscalar_axis_1])\n",
    "    \n",
    "    #put the data array and axes into a new image\n",
    "    cifti_image = nib.cifti2.cifti2.Cifti2Image(dataobj=data_array, header=cifti_hdr)\n",
    "    \n",
    "    #Save the cifti image\n",
    "    nib.save(cifti_image, output_cifti_path)\n",
    "    \n",
    "    return\n",
    "\n",
    "def format_subjects_conns_as_csv(output_path, subject_ids, connectivity_matrix, connection_headers=None):\n",
    "    \"\"\"\n",
    "    #Connectivity matrix should be formatted as subjects x connections.\n",
    "    #Connection headers specify the headers to be placed in the first row,\n",
    "    #otherwise will be auto-populated. Subject IDs will be set as first column\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_path, 'w') as output_file:\n",
    "        \n",
    "        output_file.write('subject_ids,')\n",
    "        if type(connection_headers) == type(None):\n",
    "            \n",
    "            for i in range(connectivity_matrix.shape[1]):\n",
    "                output_file.write('Connection_' + str(i) + ',')\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            \n",
    "            heading = ','.join(connection_headers) + '\\n'\n",
    "            output_file.write(heading)\n",
    "            \n",
    "            \n",
    "        for i in range(connectivity_matrix.shape[0]):\n",
    "            output_file.write(subject_ids[i] + ',')\n",
    "            temp_line = ''\n",
    "            for j in range(connectivity_matrix.shape[1]):\n",
    "                temp_line += str(connectivity_matrix[i,j]) + ',' #np.array2string(connectivity_matrix[i,:], separator=',')[1:-1]\n",
    "            output_file.write(temp_line + '\\n')\n",
    "\n",
    "def calc_variance_explained(data_matrix, trained_decomposition_object):\n",
    "    \"\"\"\n",
    "    #Function that takes as input a trained decomposition object from sklearn\n",
    "    #and data compatable with the trained decomposition object, and calculates\n",
    "    #for each component. The function outputs to variables (1)\n",
    "    #components_variance_explained which is very close to the variance_explained_\n",
    "    #property for sklearn's PCA object, and (2) components_variance_explained_ratio_\n",
    "    #which is different than sklearn's ratio property as this it is still calculated based\n",
    "    #on the total variance across examples (summed per feature), not based\n",
    "    #on the sum of variance explained for all components\n",
    "    \"\"\"\n",
    "    \n",
    "    num_components = trained_decomposition_object.components_.shape[0]\n",
    "    \n",
    "    components_variance_explained = np.zeros(num_components)\n",
    "    components_variance_explained_ratio = np.zeros(num_components)\n",
    "    \n",
    "    PCA_object = decomposition.PCA()\n",
    "    PCA_object.fit(data_matrix)\n",
    "    \n",
    "    original_variance = np.sum(np.var(data_matrix,axis=0))\n",
    "\n",
    "    comp_scores = trained_decomposition_object.transform(data_matrix)\n",
    "    \n",
    "    for component_number in range(num_components):\n",
    "    \n",
    "        nth_comp_scores = np.reshape(comp_scores[:,component_number], (data_matrix.shape[0],1))\n",
    "        nth_comp_weights = np.reshape(trained_decomposition_object.components_[component_number,:], (1,data_matrix.shape[1]))\n",
    "\n",
    "        nth_component_in_orig_space = np.matmul(nth_comp_scores,nth_comp_weights)\n",
    "        difference = data_matrix - nth_component_in_orig_space\n",
    "        subtracted_variance = np.sum(np.var(difference,axis=0))\n",
    "        components_variance_explained[component_number] = original_variance - subtracted_variance\n",
    "        \n",
    "    components_variance_explained_ratio = components_variance_explained/original_variance\n",
    "    \n",
    "    return components_variance_explained, components_variance_explained_ratio\n",
    "            \n",
    "                \n",
    "            \n",
    "def calc_connectivity_components(input_path_or_list, output_folder, parcel_ids_path=None, algorithm_type='PCA', n_components=None, z_score_prior_to_decomp=False):\n",
    "    \"\"\"\n",
    "    #Function to take subjects' connectivity data (generally region to whole-brain)\n",
    "    #and do a PCA across subjects, to produce a number of lower-dimension connectivity \n",
    "    #component 'scores' per subject. The function will save (1) subject_component_scores,\n",
    "    #(2) compoent_weights (at least as a csv, but if pscalars used also projected onto the\n",
    "    #cifti surface), (3) the amount of variance explained for each component (currently only\n",
    "    #specified for PCA), (4) a csv file that is a tabulated copy of the input connectivity\n",
    "    #data. This function only saves output, does not return anything.\n",
    "    #\n",
    "    #\n",
    "    #DISCLAIMER: Current implementation of cifti output files is limited, such that when\n",
    "    #you click on different areas of the cortex, wb_view will freeze and quit.. \n",
    "    #otherwise it works fine. Also remember the cifti file contains multiple dimensions...\n",
    "    #Also, any Inf values will be set to 0\n",
    "    #\n",
    "    #\n",
    "    #output_folder - the folder where the files generated from this function will be saved\n",
    "    # (this function will make the folder if it doesn't exist)\n",
    "    #\n",
    "    #parcel_ids_path - optional (if using *npy files), this is the path to a text file that\n",
    "    #will have one name for a connectivity edge per line\n",
    "    #\n",
    "    #algorithm_tpye - the type of sklearn algorithm to use, could be 'PCA', 'FastICA', or 'DictionaryLearning',\n",
    "    #PCA will return components that are ordered in terms of the amount of variance they explain, and\n",
    "    #'FastICA'/'DictionaryLearning' alternatively will be unordered\n",
    "    #\n",
    "    #n_components (optional) the number of components to be saved, defaults to None which returns the maximum amount.\n",
    "    # probably will want < 10. REMEMBER, THE MAXIMUM NUMBER OF COMPONENTS IS GENERALLY SET BY THE NUMBER OF SUBJECTS,\n",
    "    #IF YOU ONLY HAVE 10 SUBJECTS, YOU CAN ONLY GET 10 COMPONENTS. ALTERNATIVELY WHEN THERE ARE MORE SUBJECTS THAN\n",
    "    #CONNECTIVITY EDGES, THE NUMBER OF COMPONENTS WILL BE SET BY THE NUMBER OF CONNECTIVITY EDGES\n",
    "    #\n",
    "    #z_score_prior_to_decomp - defaults to false, optionally you can z_score each edge across\n",
    "    #subjects so that each edge is weighted equally in decomposition process\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    subj_conn_vals, parcel_labels, subjects, template_cifti_path = load_conns(input_path_or_list, parcel_ids_path=parcel_ids_path)\n",
    "    \n",
    "    #Set any edges that are equal to infinity to 0\n",
    "    subj_conn_vals[subj_conn_vals == np.inf] = 0\n",
    "    \n",
    "    #If specified, z-score conn values so that\n",
    "    #each edge is weighted evenly\n",
    "    if z_score_prior_to_decomp:\n",
    "        subj_conn_vals = zscore(subj_conn_vals,axis=0)\n",
    "    \n",
    "    if algorithm_type == 'PCA':\n",
    "        \n",
    "        decomposition_object = decomposition.PCA(n_components = n_components)\n",
    "    \n",
    "    elif algorithm_type == 'FastICA':\n",
    "        \n",
    "        decomposition_object = decomposition.FastICA(n_components = n_components)\n",
    "    \n",
    "    elif algorithm_type == 'DictionaryLearning':\n",
    "        \n",
    "        decomposition_object = decomposition.DictionaryLearning(n_components = n_components)\n",
    "        \n",
    "        \n",
    "    decomposition_object.fit(subj_conn_vals)\n",
    "    component_scores = decomposition_object.transform(subj_conn_vals)\n",
    "    \n",
    "    dimension_names = []\n",
    "    for i in range(component_scores.shape[1]):\n",
    "        dimension_names.append(algorithm_type + '_0' + str(i)) \n",
    "    \n",
    "    \n",
    "    #to do (1), make overlays, save component scores, save weights, make plot to show variance explained\n",
    "    if os.path.exists(output_folder) == False:\n",
    "        os.mkdir(output_folder)\n",
    "    \n",
    "    #Save the component scores for the subjects\n",
    "    format_subjects_conns_as_csv(os.path.join(output_folder, 'subject_component_scores.csv'), subjects, component_scores, connection_headers=dimension_names)\n",
    "\n",
    "    \n",
    "    #Save the weights for different components to a csv\n",
    "    data_array = decomposition_object.components_\n",
    "    format_subjects_conns_as_csv(os.path.join(output_folder, 'component_weights.csv'), dimension_names, data_array, connection_headers=parcel_labels)\n",
    "    \n",
    "    #Write the weights for different components to a cifti file if a template is found\n",
    "    if template_cifti_path != None:\n",
    "        make_pscalar_cifti(data_array, template_cifti_path, os.path.join(output_folder, 'component_weights.pscalar.nii'), dimension_names=dimension_names)\n",
    "        \n",
    "    \n",
    "    #Calculate variance exlained - this seems to be having some issues\n",
    "    #outside of PCA decomposition.....\n",
    "    variance_explained, variance_explained_ratio = calc_variance_explained(subj_conn_vals, decomposition_object)\n",
    "    format_subjects_conns_as_csv(os.path.join(output_folder,'components_variance_explained.csv'), ['Variance_Explained', 'Variance_Explained_Ratio'], \n",
    "                                 np.vstack((variance_explained, variance_explained_ratio)), connection_headers=dimension_names)\n",
    "    \n",
    "    #Also save the raw connectivity values\n",
    "    format_subjects_conns_as_csv(os.path.join(output_folder, 'original_conn_values.csv'), subjects, subj_conn_vals, connection_headers=parcel_labels)\n",
    "    \n",
    "    \n",
    "    #Save the settings\n",
    "    settings_dict = {'input_path_or_list' : input_path_or_list,\n",
    "                     'output_folder' : output_folder,\n",
    "                     'parcel_ids_path' : parcel_ids_path,\n",
    "                     'algorithm_type' : algorithm_type,\n",
    "                     'n_components' : n_components,\n",
    "                     'z_score_prior_to_decomp' : z_score_prior_to_decomp}\n",
    "    \n",
    "    json_object = json.dumps(settings_dict, indent=4)\n",
    "    json_path = os.path.join(output_folder, 'decomposition_settings.json')\n",
    "    with open(json_path, 'w') as temp_file:\n",
    "        temp_file.write(json_object)\n",
    "    \n",
    "    \n",
    "    return   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does this code do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this code takes connectivity data (or really any 1d map) and reduces it to a number of components specified by the user, using either PCA, Dictionary Learning, or ICA from scikit-learn for computation. This is done using the function 'calc_connectivity_components' whose usage is described above and will also be shown below. After running 'calc_connectivity_components' a new folder will be created that will contain csv files for (1) subject component scores (i.e. component strengths), (2) weights for the different components (i.e. what areas contribute to the components), (3) the amount of variance explained by the different components, (4) a json describing the settings used to run the 'calc_connectivity_components' command, (5) and a csv copy of the original connectivity data used to run the decomposition.\n",
    "\n",
    "The 'calc_connectivity_components' function supports a few different formats for input data, but if the input data are in parcellated cifti format, then the function will also output the different component weights as a cifti overlay so they can be visualized in connectome workbench."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can the input connectivity data look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'calc_connectivity_components' function takes a input variable input_or_path_list that can take a variety of forms, and that will point the function to the connectivity data. In different capacities, this supports (1) cifti pscalar files, (2) numpy arrays (*npy files), or (3) combined csv files to load connectivity data for the decomposition. If either pscalar/*npy files will be used to load connectivity data, then input_or_path_list can either be a path to a directory containing only *pscalar.nii files or *.npy files (mostly), or can be a list to the individual files you want to use. Since parcel labels can't be specified directly with numpy files, a file ending in txt can also be placed in the directory where each line has the desired (correctly ordered) label for a connectivity edge. *npy files should have shape <n_edges>. If a csv file is used, the first column should be subject_id, and there should be a heading for edge labels.\n",
    "\n",
    "As mentioned previously, component weights will only be output in cifti if the cifti input option is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What parameters do I need to specify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function to be ran is shown as:\n",
    "\n",
    "- calc_connectivity_components(input_path_or_list, output_folder, parcel_ids_path=None, algorithm_type='PCA', n_components=None, z_score_prior_to_decomp=False)\n",
    "\n",
    "The two required inputs are input_path_or_list (described above), and output_folder (which is where the results will be stored - the function creates this folder if it doesn't exist)\n",
    "\n",
    "### The remaining optional parameters are as follows:\n",
    "\n",
    "- parcel_ids_path: only for when input_path_or_list points to *npy files, this is the path to label descriptors for each connectivity edge (should be formatted one label per line)\n",
    "- algorithm_tpye: Defaults to be PCA, can also be scikit-learn's implementation of FastICA and DictionaryLearning\n",
    "- n_components: When set to None will return the maximum possible number of components (constrained by either the number of subjects or number of connectivity edges) but can alternatively be set to any given integer below the maximum (probably will be < 10 for PCA and the number for ICA/DictionaryLearning may reuqire fine tuning as different n_component values will presumably give different weights for all components)\n",
    "- z_score_prior_to_decomp: If set to true, each connectivity edge will be demeaned and variance normalized across subjects such that each edge is weighted equally in the decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The cifti files generated here have an issue where when opened in connectome workbench, if the user clicks on any of the cortical areas (i.e. to see the value of a vertex) workbench will crash... otherwise visualization seems to work fine\n",
    "- A custom function was built to test the variance explained by different components (as scikit-learn only has this feature for PCA and not FastICA or DictionaryLearning), which is numerically equivelent to scikit-learn's PCA attributes (except for purposeful deviation when n_components < max), but is giving values that seem to be incorrect for DictionaryLearning and probably also FastICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_folder_with_pscalars = '/home/lnpi15-raid7/lee-data/HCP_PREPROC_PHCPA/bilateral_thal_gsr_zcorrs_pscalars'\n",
    "path_to_where_i_want_data_saved = '/home/lnpi15-raid7/lee-data/HCP_PREPROC_PHCPA/bilateral_thal_gsr_zcorrs_pca'\n",
    "calc_connectivity_components(path_to_folder_with_pscalars, path_to_where_i_want_data_saved, n_components=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['original_conn_values.csv',\n",
       " 'component_weights.csv',\n",
       " 'decomposition_settings.json',\n",
       " 'subject_component_scores.csv',\n",
       " 'component_weights.pscalar.nii',\n",
       " 'components_variance_explained.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_to_where_i_want_data_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "ipykernel_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
